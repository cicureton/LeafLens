{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOU/G24EXB6gDri+ImYVj2Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cicureton/LeafLens/blob/ai-model/LeafLens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chPqPb8q59z_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load pretrained MobileNetV3\n",
        "mobilenet_v3_large = models.mobilenet_v3_large(weights=\"IMAGENET1K_V2\")\n",
        "\n",
        "# Remove ImageNet classifier\n",
        "mobilenet_v3_large.classifier = nn.Identity()\n",
        "\n",
        "# Freeze early layers so we just train the last convolutional layers\n",
        "for param in mobilenet_v3_large.features[:-10].parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bls0oJImAHuk",
        "outputId": "d180df46-2987-47a4-8504-b9c6d38e8a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive to store data\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Read dataset PlantClef that contains all plant species data\n",
        "df = pd.read_csv('/content/drive/MyDrive/PlantCLEF2024_single_plant_training_metadata.csv', sep=None, engine='python')\n",
        "# Print head of csv file\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "S1POHvkRAjAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a74ac2a-5b34-4806-a8ca-4ab802c7cf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                     image_name organ  species_id      obs_id  \\\n",
            "0  59feabe1c98f06e7f819f73c8246bd8f1a89556b.jpg  leaf     1396710  1008726402   \n",
            "1  dc273995a89827437d447f29a52ccac86f65476e.jpg  leaf     1396710  1008724195   \n",
            "2  416235e7023a4bd1513edf036b6097efc693a304.jpg  leaf     1396710  1008721908   \n",
            "3  cbd18fade82c46a5c725f1f3d982174895158afc.jpg  leaf     1396710  1008699177   \n",
            "4  f82c8c6d570287ebed8407cefcfcb2a51eaaf56e.jpg  leaf     1396710  1008683100   \n",
            "\n",
            "    license partner          author  altitude   latitude  longitude  \\\n",
            "0  cc-by-sa     NaN   Gulyás Bálint  205.9261  47.592160  19.362895   \n",
            "1  cc-by-sa     NaN    vadim sigaud  323.7520  47.906703   7.201746   \n",
            "2  cc-by-sa     NaN     fil escande  101.3160  48.826774   2.352774   \n",
            "3  cc-by-sa     NaN  Desiree Verver    5.1070  52.190427   6.009677   \n",
            "4  cc-by-sa     NaN      branebrane  165.3390  45.794739  15.965862   \n",
            "\n",
            "   gbif_species_id           species  genus    family   dataset publisher  \\\n",
            "0        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n",
            "1        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n",
            "2        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n",
            "3        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n",
            "4        5284517.0  Taxus baccata L.  Taxus  Taxaceae  plantnet  plantnet   \n",
            "\n",
            "                                          references  \\\n",
            "0  https://identify.plantnet.org/fr/k-southwester...   \n",
            "1  https://identify.plantnet.org/fr/k-southwester...   \n",
            "2  https://identify.plantnet.org/fr/k-southwester...   \n",
            "3  https://identify.plantnet.org/fr/k-southwester...   \n",
            "4  https://identify.plantnet.org/fr/k-southwester...   \n",
            "\n",
            "                                                 url learn_tag  \\\n",
            "0  https://bs.plantnet.org/image/o/59feabe1c98f06...     train   \n",
            "1  https://bs.plantnet.org/image/o/dc273995a89827...     train   \n",
            "2  https://bs.plantnet.org/image/o/416235e7023a4b...     train   \n",
            "3  https://bs.plantnet.org/image/o/cbd18fade82c46...     train   \n",
            "4  https://bs.plantnet.org/image/o/f82c8c6d570287...     train   \n",
            "\n",
            "                                    image_backup_url  \n",
            "0  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
            "1  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
            "2  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
            "3  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n",
            "4  https://lab.plantnet.org/LifeCLEF/PlantCLEF202...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print columns of the datset (features)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyDUvPr2UDDe",
        "outputId": "695493e6-c093-4a06-f74d-2d29b94aebbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image_name', 'organ', 'species_id', 'obs_id', 'license', 'partner',\n",
            "       'author', 'altitude', 'latitude', 'longitude', 'gbif_species_id',\n",
            "       'species', 'genus', 'family', 'dataset', 'publisher', 'references',\n",
            "       'url', 'learn_tag', 'image_backup_url'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to work with a subset of the data and take the first 20 different species\n",
        "# Select column species in our csv file and collect each unique species for first [:20] items\n",
        "species_list = df['species'].unique()[:20]\n",
        "for species in species_list:\n",
        "  print(species)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeySX8ZNURUu",
        "outputId": "879e793b-d568-4986-899f-63f944495712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taxus baccata L.\n",
            "Dryopteris filix-mas (L.) Schott\n",
            "Roemeria hybrida (L.) DC.\n",
            "Rosa corymbifera Borkh.\n",
            "Potentilla reptans L.\n",
            "Saponaria ocymoides L.\n",
            "Rumex crispus L.\n",
            "Salix alba L.\n",
            "Salvia microphylla Kunth\n",
            "Prunus cerasus L.\n",
            "Ranunculus repens L.\n",
            "Salix caprea L.\n",
            "Potentilla indica (Andrews) Th.Wolf\n",
            "Melilotus albus Medik.\n",
            "Rhus typhina L.\n",
            "Solanum nigrum L.\n",
            "Primula vulgaris Huds.\n",
            "Schoenoplectus lacustris (L.) Palla\n",
            "Podranea ricasoliana (Tanfani) Sprague\n",
            "Quercus cerris L.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Store our subset_metadata dataset so it can be written to csv file\n",
        "# Take the subset in our original dataset (df) that contains columns [species] that exist in (species_list)\n",
        "# to get our subset_metadata\n",
        "\n",
        "subset_metadata = df[df['species'].isin(species_list)]"
      ],
      "metadata": {
        "id": "UB8bqlXjcyfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create save path for my new subset dataset\n",
        "save_path = '/content/drive/MyDrive/subset_metadata.csv'\n",
        "\n",
        "# Check if already exists so don't have to overwrite every time\n",
        "if not os.path.exists(save_path):\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(\"File saved\")\n",
        "else:\n",
        "    print(\"File already exists.\")"
      ],
      "metadata": {
        "id": "7Qy2Fre5Vbz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85f95c3-d8c2-4cab-9410-def8024a8bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check subset_metadata information (20 species, 8450 unique images)\n",
        "print(subset_metadata.shape)\n",
        "print(subset_metadata.nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx0CY5byXN4M",
        "outputId": "bab6bcf8-be34-4a8b-8d13-11f19d79c624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8450, 20)\n",
            "image_name          8450\n",
            "organ                  7\n",
            "species_id            20\n",
            "obs_id              8037\n",
            "license                4\n",
            "partner                3\n",
            "author              6294\n",
            "altitude            3744\n",
            "latitude            5302\n",
            "longitude           5306\n",
            "gbif_species_id       20\n",
            "species               20\n",
            "genus                 18\n",
            "family                16\n",
            "dataset                2\n",
            "publisher              2\n",
            "references          8017\n",
            "url                 8450\n",
            "learn_tag              3\n",
            "image_backup_url    8450\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "# Here we are giong to download our images locally so we don't have to use the image url's\n",
        "\n",
        "# Create directory to store my subset dataset images\n",
        "os.makedirs('/content/drive/MyDrive/subset_images', exist_ok=True)\n",
        "\n",
        "# Function to download images\n",
        "def download_image(url):\n",
        "      # filename holds the new filename by joining/getting rid of the https:// and simply\n",
        "      # stores the image name from the url (i.e https://plus.unsplash.com/premium_photo-1757260019141 becomes -->\n",
        "      # premium_photo-1752... and gets stored in subset_images)\n",
        "      filename = os.path.join(\"/content/drive/MyDrive/subset_images\", os.path.basename(url))\n",
        "      # Check if saved so we don't overwrite every time\n",
        "      if not os.path.exists(filename):\n",
        "          # Get url\n",
        "          r = requests.get(url, timeout=10)\n",
        "          if r.status_code == 200:\n",
        "            # Get request for image and store in r\n",
        "            r = requests.get(url)\n",
        "            # Write the image (r) as binary and store in subset_images\n",
        "            with open(filename, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "\n",
        "# list all the urls in subset_metadata to download\n",
        "urls = subset_metadata[\"url\"].tolist()\n",
        "\n",
        "# Increase # of threads operating the downloads (26) and use tqdm to initialize a loading bar\n",
        "# downlad_image function for all urls in our list\n",
        "with ThreadPoolExecutor(max_workers=26) as executor:\n",
        "    list(tqdm(executor.map(download_image, urls), total=len(urls)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9TJEvm2YGq7",
        "outputId": "c16bf01c-da70-4bb8-dabb-fddb0d2d1256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8450/8450 [00:00<00:00, 11366.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "from sklearn.model_selection import train_test_split\n",
        "# initialize train/test/val split for our model\n",
        "\n",
        "# initialize train/val and test to split our subset metadata into 80% and 20%\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(subset_metadata[\"url\"], subset_metadata[\"species\"], test_size=0.2, random_state=42, stratify=subset_metadata[\"species\"])\n",
        "# split train/val into 60/20 by giving validation size 25% of 80 = 20\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
        "\n",
        "\n",
        "print(f\"Train size: {len(X_train)}\")\n",
        "print(f\"Validation size: {len(X_val)}\")\n",
        "print(f\"Test size: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "sj1M4WlSi4m-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660732ce-1ef9-4280-9b07-5501342ea6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 5070\n",
            "Validation size: 1690\n",
            "Test size: 1690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_folder = \"/content/drive/MyDrive/subset_images\"\n",
        "# Change the datasets to correct localpaths (because i split the datasets using the url attribute) to local filepath (otherwise error will occur when trying to train model (could not find https://lasjfldja))\n",
        "train_images = [os.path.join(image_folder, os.path.basename(url)) for url in X_train]\n",
        "val_images   = [os.path.join(image_folder, os.path.basename(url)) for url in X_val]\n",
        "test_images  = [os.path.join(image_folder, os.path.basename(url)) for url in X_test]\n",
        "\n",
        "# Reset index back to sequential because after splitting train-val-test they may not be sequential anymore\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_val   = y_val.reset_index(drop=True)\n",
        "y_test  = y_test.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "Hl8I1UGuXOd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "# Create a custom dataset to house our training data (pytorch models needs tensors not csv/url files which the images are currently in)\n",
        "class PlantDatasetLocal(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(self.labels.unique())\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.image_directory = \"/content/drive/MyDrive/subset_images\" # Add image directory\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "# How to get individual items in the plantdatasetLocal\n",
        "    def __getitem__(self, idx):\n",
        "        # Construct local file path\n",
        "        image_name = os.path.basename(self.image_paths[idx])\n",
        "        image_path = os.path.join(self.image_directory, image_name)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        label = self.class_to_idx[self.labels[idx]]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "z7kAorVo-vdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "# Cleaning train images (found some of them to be missing/corrupted)\n",
        "cleaned_train_images = []\n",
        "cleaned_y_train = []\n",
        "\n",
        "# Create for loop and initialize tqdm loader to go through all train_images and open/append them to new clean list\n",
        "for path, label in tqdm(zip(train_images, y_train), total=len(train_images), desc=\"Checking train images\"):\n",
        "    try:\n",
        "        Image.open(path)\n",
        "        cleaned_train_images.append(path)\n",
        "        cleaned_y_train.append(label)\n",
        "    except UnidentifiedImageError as e:\n",
        "        print(f\"Error opening image {path}: {e} + {label}\")\n",
        "\n",
        "# Updated to new cleaned list\n",
        "train_images = cleaned_train_images\n",
        "y_train = cleaned_y_train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd3vo6FEVwj-",
        "outputId": "7972c84e-842e-4711-e176-c0aa598195f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking train images:  25%|██▍       | 1261/5070 [04:19<16:13,  3.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error opening image /content/drive/MyDrive/subset_images/70617873.jpg: cannot identify image file '/content/drive/MyDrive/subset_images/70617873.jpg' + Rosa corymbifera Borkh.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking train images: 100%|██████████| 5070/5070 [17:16<00:00,  4.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "# Cleaning validation images\n",
        "cleaned_val_images = []\n",
        "cleaned_y_val = []\n",
        "\n",
        "# Open the image to see if it is corrupted or missing, if opens then put into new list\n",
        "for path, label in tqdm(zip(val_images, y_val), total=len(val_images), desc=\"Checking validation images\"):\n",
        "    try:\n",
        "        Image.open(path)\n",
        "        cleaned_val_images.append(path)\n",
        "        cleaned_y_val.append(label)\n",
        "    except UnidentifiedImageError as e:\n",
        "        print(f\"Error opening image {path}: {e} + {label}\")\n",
        "\n",
        "# Replace original lists with cleaned ones\n",
        "val_images = cleaned_val_images\n",
        "y_val = cleaned_y_val\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQXnKmTH0xko",
        "outputId": "8cda56fd-3214-4e0c-ebc1-1d5c76eb3e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking validation images: 100%|██████████| 1690/1690 [05:47<00:00,  4.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data transformation (how the model can intepret the data. Must be 224x224 and tensor object 0-1 instead of 0-255)\n",
        "# and normalized as well\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Our labels must be reset to pd.series and reset index after changing them to\n",
        "# python lists when we cleaned them for missing/corrupted data\n",
        "y_train = pd.Series(y_train).reset_index(drop=True)\n",
        "y_val = pd.Series(y_val).reset_index(drop=True)\n",
        "y_test = pd.Series(y_test).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Create the custom train dataset using torchvision to transform our data into something apprehendable by the model (not csv, not just img urls)\n",
        "train_dataset = PlantDatasetLocal(train_images, y_train, transform=transform)\n",
        "val_dataset = PlantDatasetLocal(val_images, y_val, transform=transform)\n",
        "test_dataset = PlantDatasetLocal(test_images, y_test, transform=transform)\n",
        "# Create the train loader with a batch size of 32 that we will pass through the moddle each time, shuffled.\n",
        "# Increased the num_workers to try and increase the speed\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "5IfTnOw06z1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6368d4f2-d6aa-4f09-a8fd-0f63d4717cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3b685bd",
        "outputId": "0d0736d1-ae20-494e-8427-d376e6047e0d"
      },
      "source": [
        "# Determine the number of unique species\n",
        "num_species = len(train_dataset.classes)\n",
        "print(f\"Number of unique species: {num_species}\")\n",
        "\n",
        "# Replace the classifier layer that came with the pretrained model\n",
        "mobilenet_v3_large.classifier[3] = nn.Linear(mobilenet_v3_large.classifier[3].in_features, num_species)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique species: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Loss function and optimizier\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mobilenet_v3_large.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop using 7 epochs (seven run throughs of the same dataset)\n",
        "num_epochs = 7\n",
        "mobilenet_v3_large.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass calculates the output by feeding input thorugh model nad loss using criterion function\n",
        "        outputs = mobilenet_v3_large(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wTzL8uj_H5X",
        "outputId": "b9e977f5-e5df-45eb-a9a4-91ad1c31aaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/7: 100%|██████████| 159/159 [08:56<00:00,  3.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/7], Loss: 1.5601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/7: 100%|██████████| 159/159 [09:15<00:00,  3.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/7], Loss: 0.8187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/7: 100%|██████████| 159/159 [09:23<00:00,  3.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/7], Loss: 0.5348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/7: 100%|██████████| 159/159 [09:01<00:00,  3.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/7], Loss: 0.4131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/7: 100%|██████████| 159/159 [08:38<00:00,  3.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/7], Loss: 0.3717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/7: 100%|██████████| 159/159 [08:58<00:00,  3.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/7], Loss: 0.2809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/7: 100%|██████████| 159/159 [09:02<00:00,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/7], Loss: 0.2244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(mobilenet_v3_large.state_dict(), \"/content/drive/MyDrive/mobilenet_v3_large_plants.pth\")\n",
        "print(\"Model saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufx7yOkczxjv",
        "outputId": "5128b04a-e502-4da6-9b42-25083d5f1353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "mobilenet_v3_large.eval()  # evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "# No gradient calculation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(val_loader, desc=\"Validating\", total=len(val_loader)):\n",
        "        outputs = mobilenet_v3_large(inputs)\n",
        "        # Get class with highest score\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RSzuPR90QWb",
        "outputId": "ccae8eb5-f7f7-4fc8-fc75-caa72e4c2afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 53/53 [00:48<00:00,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 64.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXC49N7HKQj8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}